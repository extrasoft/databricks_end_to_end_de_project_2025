{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1466ed36-5782-4c91-b6c8-dc52ce3ac9cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Copy ABFS PATH\n",
    "source = \"abfss://source@dlsdatabrickseteteamea.dfs.core.windows.net\"\n",
    "bronze = \"abfss://bronze@dlsdatabrickseteteamea.dfs.core.windows.net\"\n",
    "silver = \"abfss://silver@dlsdatabrickseteteamea.dfs.core.windows.net\"\n",
    "gold = \"abfss://gold@dlsdatabrickseteteamea.dfs.core.windows.net\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "546c716c-98ed-4b38-9067-df112980e85e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Test Reading data from source**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27108570-885d-4a45-ab72-3d385136b3ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import current_timestamp\n",
    "df = spark.read.format(\"parquet\").load(f\"{source}/orders\" )\n",
    "df.count()\n",
    "# df = df.withColumn(\"test_timestamp\", current_timestamp())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7e81fbe-8d2e-4254-99d7-127710383b81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Data Reading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2836fc05-9aba-4908-8d16-f177d3a96e76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell ที่ 3 มีการอ่านไฟล์จาก container source/orders โดยใช้ Spark Structured Streaming และ Auto Loader เพื่อเก็บ Checkpoint ของไฟล์ที่ได้อ่านแล้ว ซึ่งจะช่วยให้ไม่ต้องอ่านไฟล์เดิมซ้ำ\n",
    "# readStream ใช้สำหรับการอ่านข้อมูลแบบสตรีมมิ่ง ซึ่งช่วยให้สามารถประมวลผลข้อมูลที่เข้ามาใหม่ได้อย่างต่อเนื่อง\n",
    "# cloudFiles เป็นฟอร์แมตที่ใช้กับ Auto Loader ของ Databricks ซึ่งช่วยในการจัดการไฟล์ที่เข้ามาใหม่โดยอัตโนมัติ และเก็บ Checkpoint เพื่อไม่ให้เกิดการอ่านไฟล์ซ้ำ\n",
    "\n",
    "df = spark.readStream.format(\"cloudFiles\")\\\n",
    "    .option(\"cloudFiles.format\", \"parquet\")\\\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{bronze}/checkpoint_orders\") \\\n",
    "    .load(f\"{source}/orders\")\n",
    "\n",
    "# ผลลัพธ์ที่ได้จะอยู่ในโฟล์เดอร์ bronze/checkpoint_orders/_schemas.*\n",
    "# df = df.withColumn(\"test_timestamp\", current_timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ac3ab87-9b9c-4f79-8695-1207079b775f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Data Writing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65b5996b-b382-427a-b216-c85a6a336192",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# .format(\"delta\") = เขียนเป็น Delta Table (Manage Table)\n",
    "# .format(\"parquet\") = เขียนเป็นไฟล์ Delta Parquet\n",
    "# .trigger(processingTime=\"10 seconds\")    ให้เขียนทุกๆ 10 วินาที\n",
    "# .trigger(once=True) เขียนเพียงครั้งเดียวแล้วจบ\n",
    "parquet_stream = df.writeStream.format(\"parquet\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\" , f\"{bronze}/checkpoint_orders\") \\\n",
    "    .option(\"path\" , f\"{bronze}/orders\" ) \\\n",
    "    .trigger(once=True) \\\n",
    "    .start()\n",
    "\n",
    "# ผลลัพธ์ที่ได้คือ\n",
    "# 1. Folder bronze/orders ที่มีข้อมูลเป็นไฟล์ parquet \n",
    "# 2. Folder btonze/checkpoint_orders/\n",
    "#   - __tmp_path_dir.*: ไฟล์ชั่วคราวที่ใช้ในกระบวนการเขียนข้อมูล\n",
    "#   - commits.*: ไฟล์ที่เก็บข้อมูลเกี่ยวกับการ commit ของแต่ละ batch ที่ถูกประมวลผล\n",
    "#   - metadata: ไฟล์ที่เก็บข้อมูลเมตาของการสตรีมมิ่ง เช่น schema ของ DataFrame\n",
    "#   - offsets.*: ไฟล์ที่เก็บข้อมูลเกี่ยวกับ offset ของแต่ละ batch ที่ถูกประมวลผล\n",
    "#   - source.*: ไฟล์ที่เก็บข้อมูลเกี่ยวกับแหล่งข้อมูลที่ถูกอ่าน\n",
    "\n",
    "# ถ้าเกิดมีไฟล์ใหม่เพิ่มเข้ามา ผลลัพธ์ที่ได้คือ\n",
    "# 1. Folder bronze/orders ที่มีข้อมูลเป็นไฟล์ parquet ที่เพิ่มขึ้นใหม่\n",
    "# 2. Folder btonze/checkpoint_orders/\n",
    "#   - commits: ไฟล์ 1 เพิ่มขึ้นมา\n",
    "#   - offsets: ไฟล์ 1 เพิ่มขึ้นมา\n",
    "#   - source/0/rocksdb: มีการเพิ่มขึ้นมาหลายไฟล์"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4217cb1-3c27-4ed3-bf43-63b0b1bc9100",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**การที่ Autoloader สามารถ process เฉพาะไฟล์ใหม่ที่เข้ามา เพราะว่า Rocksdb ซึ่งมีหน้าที่ในการ capture data**\n",
    "\n",
    "**Path:\tcheckpoint_orders/sources/0/rocksdb**"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze_Layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
