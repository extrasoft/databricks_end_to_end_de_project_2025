{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "640ed912-81b3-41ed-8909-bb8e8b26f7c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**3. Best Practice ในการเปรียบเทียบด้วย Hashing**\n",
    "\n",
    "วิธีการ Hash เป็นวิธีที่ มีประสิทธิภาพสูงมาก ในการตรวจสอบการเปลี่ยนแปลงของหลายๆ คอลัมน์พร้อมกัน เพราะคุณลดการเปรียบเทียบจาก N คอลัมน์ เหลือเพียง 1 คอลัมน์ (ค่า Hash)\n",
    "\n",
    "ขั้นตอน:\n",
    "1. ระบุ SCD Type 2 Attributes: กำหนดให้ชัดเจนว่าคอลัมน์ใดบ้างที่จะใช้ในการตรวจสอบการเปลี่ยนแปลง\n",
    "2. สร้าง Hash Column ใน Source Data:\n",
    "    - รวมค่าของ SCD Type 2 Attributes ทั้งหมด ใน Source Data เข้าด้วยกันเป็น String เดียว\n",
    "\n",
    "    - ใช้ฟังก์ชัน Hash (เช่น md5, sha2) เพื่อสร้าง Hash Value ของ String นั้น\n",
    "\n",
    "    - สำคัญ: ตรวจสอบให้แน่ใจว่าการรวม String จัดการ NULL values ได้อย่างสอดคล้องกัน (เช่น coalesce(column, 'NULL_VALUE_PLACEHOLDER')) และเรียงลำดับคอลัมน์ที่จะรวมเหมือนกันทุกครั้ง"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b5de7c9-431a-46ea-8de1-d732df80f4c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws, md5, col, lit, current_date\n",
    "\n",
    "# สมมติ source_df มีคอลัมน์ CustomerID, CustomerName, CustomerAddress, CustomerPhone\n",
    "# และเราจะติดตาม CustomerName, CustomerAddress, CustomerPhone\n",
    "\n",
    "# คอลัมน์ที่จะใช้สร้าง Hash (SCD Type 2 Attributes)\n",
    "scd_cols = [\"CustomerName\", \"CustomerAddress\", \"CustomerPhone\"]\n",
    "\n",
    "# สร้าง Hash Column ใน Source\n",
    "source_df_hashed = source_df.withColumn(\n",
    "    \"scd_hash\",\n",
    "    md5(concat_ws(\"||\", *[col(c) for c in scd_cols]))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "890c6e8e-5cab-4694-8c56-2d88c0d1f123",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**คำแนะนำ: ใช้ concat_ws เพื่อรวม String โดยใช้ตัวคั่นที่ไม่น่าจะปรากฏในข้อมูลจริง (เช่น ||) และถ้ามี NULL values ควรใช้ coalesce หรือ nvl เพื่อแทนที่ NULL ด้วยค่า placeholder ที่สอดคล้องกัน เช่น coalesce(col(\"CustomerAddress\"), lit(\"NULL_ADDRESS\")) เพื่อให้ Hash ไม่เปลี่ยนเมื่อค่า Null เปลี่ยน**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "741fa708-b632-4615-8dfe-2abd713aeb2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**3. เตรียม Destination Data:**\n",
    "\n",
    "- โหลด Destination Dimension Table (เช่น dim_customer)\n",
    "- กรองเฉพาะ Record ปัจจุบัน (IsCurrent = True)\n",
    "- สร้าง Hash Column สำหรับ Record ปัจจุบันเหล่านี้ด้วย Logic เดียวกันกับ Source Data (ใช้ SCD Type 2 Attributes เดียวกันและวิธี Hash เดียวกัน)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d83a73a6-049a-4531-a353-52c325ff69c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# โหลด Dimension Table\n",
    "dim_customer_table = DeltaTable.forName(spark, \"dim_customer\")\n",
    "dim_customer_df = dim_customer_table.toDF().filter(col(\"IsCurrent\") == True)\n",
    "\n",
    "# สร้าง Hash Column ใน Destination (Current Records)\n",
    "dim_customer_current_hashed = dim_customer_df.withColumn(\n",
    "    \"scd_hash\",\n",
    "    md5(concat_ws(\"||\", *[col(c) for c in scd_cols])) # ใช้ scd_cols เดียวกัน\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9bd8dac-3bf7-439d-9b21-13eaa4389105",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**4. หาประเภทของ Record (New, Changed, Unchanged):**\n",
    "\n",
    "- New Records: LEFT ANTI JOIN Source Hashed กับ Destination Hashed (โดยใช้ Business Key)\n",
    "\n",
    "- Changed Records: INNER JOIN Source Hashed กับ Destination Hashed (โดยใช้ Business Key) และ Source.scd_hash != Destination.scd_hash\n",
    "\n",
    "- Unchanged Records: INNER JOIN Source Hashed กับ Destination Hashed (โดยใช้ Business Key) และ Source.scd_hash == Destination.scd_hash (คุณสามารถ DO NOTHING กับกลุ่มนี้ หรือ UPDATE LastUpdatedTimestamp ถ้าจำเป็น)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b95b8bd1-de91-4645-8a17-0a804e89070d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# 1. New Records (Records ใน Source ที่ไม่มีใน Destination Current)\n",
    "new_records = source_df_hashed.join(\n",
    "    dim_customer_current_hashed,\n",
    "    source_df_hashed.CustomerID == dim_customer_current_hashed.CustomerID,\n",
    "    \"left_anti\"\n",
    ")\n",
    "\n",
    "# 2. Changed Records (Records ที่ Business Key ตรงกัน แต่ Hash ไม่ตรง)\n",
    "changed_records = source_df_hashed.join(\n",
    "    dim_customer_current_hashed,\n",
    "    (source_df_hashed.CustomerID == dim_customer_current_hashed.CustomerID) & \\\n",
    "    (source_df_hashed.scd_hash != dim_customer_current_hashed.scd_hash),\n",
    "    \"inner\"\n",
    ").select(source_df_hashed[\"*\"], dim_customer_current_hashed.SK.alias(\"old_sk\")) # เอา SK เก่ามาด้วยเพื่ออัปเดต"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5a28246-06e7-4015-a53e-2e4a2c582529",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**ดำเนินการ MERGE INTO (ใช้ Delta Lake):**\n",
    "\n",
    "นี่คือส่วนที่มีประสิทธิภาพที่สุดในการจัดการ SCD Type 2 ด้วย PySpark บน Databricks เพราะ Delta Lake MERGE รองรับทั้ง UPDATE และ INSERT ในคำสั่งเดียว"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f7d1321-a062-4fae-b7f1-7d041559141c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import current_date, date_sub\n",
    "\n",
    "# โหลด Dimension Table เป็น DeltaTable object\n",
    "dim_customer_delta_table = DeltaTable.forName(spark, \"dim_customer\")\n",
    "\n",
    "# กำหนดวันที่ปัจจุบันสำหรับการทำ SCD Type 2\n",
    "current_date_val = current_date()\n",
    "yesterday_date_val = date_sub(current_date(), 1)\n",
    "\n",
    "# 1. อัปเดต Record เก่า (สำหรับ Changed Records)\n",
    "# เราจะใช้ changed_records_to_update เป็น DataFrame ที่มีคอลัมน์ SK ของ Record เก่าที่ต้องเปลี่ยน EndDate\n",
    "# ในตัวอย่างนี้ เราจะ MERGE โดยใช้ Business Key จาก changed_records\n",
    "# และใน WHEN MATCHED เราจะอัปเดต Record เก่าให้ EndDate = current_date - 1 และ IsCurrent = False\n",
    "# และ INSERT Record ใหม่\n",
    "# นี่คือขั้นตอนที่ซับซ้อนที่สุดใน MERGE สำหรับ SCD Type 2\n",
    "\n",
    "# เตรียม Source DataFrame สำหรับ MERGE\n",
    "# Source สำหรับ MERGE จะรวม New Records และ Changed Records ใหม่\n",
    "# โดยเพิ่มคอลัมน์ metadata ที่จำเป็นสำหรับ SCD Type 2\n",
    "# และคอลัมน์สำหรับ Join กับ dim_customer เพื่อหา old_sk สำหรับการอัปเดต\n",
    "\n",
    "# สำหรับ New Records: เพิ่มคอลัมน์ SCD metadata\n",
    "new_records_for_merge = new_records.withColumn(\"StartDate\", current_date_val) \\\n",
    "                                   .withColumn(\"EndDate\", lit(\"9999-12-31\").cast(\"date\")) \\\n",
    "                                   .withColumn(\"IsCurrent\", lit(True)) \\\n",
    "                                   .withColumn(\"scd_action\", lit(\"NEW\")) # เพิ่มคอลัมน์ระบุ Action\n",
    "\n",
    "# สำหรับ Changed Records: เพิ่มคอลัมน์ SCD metadata สำหรับ Record ใหม่\n",
    "# และระบุ Old SK เพื่อใช้ในการอัปเดต Record เก่า\n",
    "changed_records_for_merge = changed_records.withColumn(\"StartDate\", current_date_val) \\\n",
    "                                           .withColumn(\"EndDate\", lit(\"9999-12-31\").cast(\"date\")) \\\n",
    "                                           .withColumn(\"IsCurrent\", lit(True)) \\\n",
    "                                           .withColumn(\"scd_action\", lit(\"CHANGED\")) # เพิ่มคอลัมน์ระบุ Action\n",
    "\n",
    "# รวม New และ Changed records เพื่อเป็น Source เดียวสำหรับ MERGE\n",
    "# ต้องแน่ใจว่ามีคอลัมน์ครบถ้วนตามที่ Dimension Table คาดหวัง\n",
    "# ในทางปฏิบัติ เราจะสร้าง DataFrame ที่รวมข้อมูลที่จะถูก INSERT (New records และ New version of changed records)\n",
    "# และอีก DataFrame ที่ระบุข้อมูลที่จะถูก UPDATE (Old version of changed records)\n",
    "\n",
    "# วิธีการทั่วไปในการใช้ MERGE สำหรับ SCD Type 2:\n",
    "# 1. สร้าง Temp View ของ source_df_hashed\n",
    "# 2. ทำ MERGE INTO destination_table AS target USING source_df_hashed AS source\n",
    "#    ON target.BusinessKey = source.BusinessKey AND target.IsCurrent = TRUE\n",
    "#    WHEN MATCHED AND target.scd_hash != source.scd_hash THEN  -- กรณีเปลี่ยนแปลง\n",
    "#        UPDATE SET EndDate = current_date - 1, IsCurrent = FALSE -- อัปเดต Record เดิม\n",
    "#    WHEN NOT MATCHED THEN -- กรณี Record ใหม่\n",
    "#        INSERT * -- ใส่ Record ใหม่\n",
    "# 3. ต้องรัน MERGE อีกครั้งเพื่อ INSERT Record ใหม่สำหรับ Changed Records\n",
    "\n",
    "# ด้วย Databricks Delta Lake และ PySpark, Best Practice คือใช้ Python API ของ DeltaTable\n",
    "# ซึ่งช่วยให้จัดการ Logic ได้ง่ายขึ้น\n",
    "\n",
    "# Step 1: สร้าง DataFrame ที่รวมข้อมูลที่จะถูก 'MERGE' เข้าไปใน Dimension Table\n",
    "# นี่คือ Source DataFrame ที่จะใช้ในคำสั่ง MERGE\n",
    "# มันควรจะมีข้อมูลสำหรับทั้ง NEW RECORDS และ NEW VERSION ของ CHANGED RECORDS\n",
    "# และคอลัมน์ที่จำเป็นสำหรับการระบุ OLD RECORDS เพื่อ UPDATE (เช่น Business Key)\n",
    "# และคอลัมน์ metadata สำหรับ SCD Type 2 (StartDate, EndDate, IsCurrent)\n",
    "source_df_for_merge = source_df_hashed.select(\n",
    "    col(\"CustomerID\"), # Business Key\n",
    "    col(\"CustomerName\"), col(\"CustomerAddress\"), col(\"CustomerPhone\"), # SCD Type 2 Attributes\n",
    "    col(\"scd_hash\") # Hash\n",
    "    # ... เพิ่มคอลัมน์อื่นๆ ที่เป็น Non-SCD attributes ด้วย\n",
    ")\n",
    "\n",
    "# ทำ MERGE\n",
    "dim_customer_delta_table.alias(\"target\").merge(\n",
    "    source_df_for_merge.alias(\"source\"),\n",
    "    \"\"\"target.CustomerID = source.CustomerID AND target.IsCurrent = TRUE\"\"\" # เงื่อนไข Join\n",
    ") \\\n",
    ".whenMatchedUpdate(\n",
    "    condition = \"target.scd_hash != source.scd_hash\", # ถ้า Hash ไม่ตรง (มีการเปลี่ยนแปลง)\n",
    "    set = { # อัปเดต Record เก่า\n",
    "        \"EndDate\": f\"CAST('{current_date_val}' AS DATE) - INTERVAL 1 DAY\", # ตั้ง EndDate เป็นวันก่อนหน้า\n",
    "        \"IsCurrent\": \"FALSE\"\n",
    "    }\n",
    ") \\\n",
    ".whenNotMatchedInsert(\n",
    "    values = { # Insert Record ใหม่ (สำหรับ New Records)\n",
    "        \"CustomerID\": \"source.CustomerID\",\n",
    "        \"CustomerName\": \"source.CustomerName\",\n",
    "        \"CustomerAddress\": \"source.CustomerAddress\",\n",
    "        \"CustomerPhone\": \"source.CustomerPhone\",\n",
    "        # ... เพิ่มคอลัมน์อื่นๆ\n",
    "        \"scd_hash\": \"source.scd_hash\",\n",
    "        \"StartDate\": f\"CAST('{current_date_val}' AS DATE)\",\n",
    "        \"EndDate\": \"CAST('9999-12-31' AS DATE)\",\n",
    "        \"IsCurrent\": \"TRUE\"\n",
    "    }\n",
    ") \\\n",
    ".execute()\n",
    "\n",
    "# **สำคัญ:** MERGE API ของ DeltaTable มีข้อจำกัดเล็กน้อยสำหรับ SCD Type 2 แบบมาตรฐาน\n",
    "# ที่ต้อง UPDATE แถวเดิม และ INSERT แถวใหม่ในรอบเดียวกัน\n",
    "# คุณอาจต้องรัน MERGE สองครั้ง หรือใช้วิธีที่ซับซ้อนขึ้น\n",
    "# หรือใช้วิธีที่นิยมคือ:\n",
    "# Step 1: ระบุ Records ที่ต้อง 'Expire' (Current Records ที่มีการเปลี่ยนแปลง)\n",
    "# Step 2: ระบุ Records ที่ต้อง 'Insert' (New Records + New Version ของ Changed Records)\n",
    "# Step 3: รัน DeltaTable.update() สำหรับ Step 1\n",
    "# Step 4: รัน DeltaTable.insert() สำหรับ Step 2\n",
    "# หรือใช้ SQL MERGE INTO หลายเงื่อนไข (ซับซ้อนกว่า PySpark API)\n",
    "\n",
    "# **วิธีการที่แนะนำและชัดเจนกว่าสำหรับการทำ SCD Type 2 โดยเฉพาะ**\n",
    "# คือการใช้การ Join และการเขียนผลลัพธ์ลง Delta Table แบบ Two-step MERGE/UPDATE-INSERT\n",
    "# หรือใช้ Logic ที่ซับซ้อนขึ้นใน Single MERGE\n",
    "\n",
    "# Logic ที่นิยมทำสำหรับการ MERGE SCD Type 2 ใน Databricks:\n",
    "# 1. ระบุ Record ที่มีอยู่ใน Source แต่ไม่มีใน Target (New)\n",
    "# 2. ระบุ Record ที่มีอยู่ใน Source และ Target และ Hash ไม่ตรงกัน (Changed)\n",
    "# 3. ระบุ Record ที่มีอยู่ใน Source และ Target และ Hash ตรงกัน (Unchanged)\n",
    "# 4. สร้าง DataFrame ที่ประกอบด้วย:\n",
    "#    a. New Records (จาก 1.)\n",
    "#    b. New Version of Changed Records (จาก 2.) - มี StartDate = current_date, EndDate = Sentinel, IsCurrent = True\n",
    "# 5. ใช้ SQL MERGE หรือ DeltaTable.merge() เพื่อ:\n",
    "#    a. เมื่อ Business Key ตรงกันและ IsCurrent = TRUE:\n",
    "#       i. ถ้า Hash ไม่ตรง: UPDATE Record ใน Dimension Table นั้นให้ IsCurrent = FALSE และ EndDate = current_date - 1\n",
    "#       ii. ถ้า Hash ตรง: DO NOTHING หรือ UPDATE LastUpdatedTimestamp\n",
    "#    b. เมื่อ Business Key ไม่ตรง: INSERT Record (จาก 4.a และ 4.b)\n",
    "\n",
    "# เนื่องจาก PySpark DeltaTable.merge() มี .whenMatchedUpdate() และ .whenNotMatchedInsert()\n",
    "# มันไม่สามารถทำ 'Update' และ 'Insert' Record ใหม่ของสิ่งที่ 'Matched' และ 'Changed' ได้ในคำสั่งเดียว\n",
    "# ดังนั้น สิ่งที่นิยมทำคือ:\n",
    "\n",
    "# 1. หา Records ที่มีการเปลี่ยนแปลงและต้อง 'Expired'\n",
    "records_to_expire = dim_customer_current_hashed.join(\n",
    "    source_df_hashed,\n",
    "    (dim_customer_current_hashed.CustomerID == source_df_hashed.CustomerID) & \\\n",
    "    (dim_customer_current_hashed.scd_hash != source_df_hashed.scd_hash),\n",
    "    \"inner\"\n",
    ").select(dim_customer_current_hashed.SK) # เอา SK ของ Record เก่าที่ต้องเปลี่ยน EndDate\n",
    "\n",
    "# 2. อัปเดต Record เก่าให้ 'Expired'\n",
    "if records_to_expire.count() > 0:\n",
    "    dim_customer_delta_table.alias(\"target\").merge(\n",
    "        records_to_expire.alias(\"source\"),\n",
    "        \"target.SK = source.SK\"\n",
    "    ) \\\n",
    "    .whenMatchedUpdate(\n",
    "        set = {\n",
    "            \"EndDate\": f\"CAST('{current_date_val}' AS DATE) - INTERVAL 1 DAY\",\n",
    "            \"IsCurrent\": \"FALSE\"\n",
    "        }\n",
    "    ) \\\n",
    "    .execute()\n",
    "\n",
    "# 3. หา Records ที่เป็น 'New' หรือ 'Changed' (เวอร์ชันใหม่) ที่ต้อง 'Insert'\n",
    "#    New: Business Key ไม่มีใน dim_customer_current_hashed\n",
    "#    Changed: Business Key มีใน dim_customer_current_hashed แต่ hash ไม่ตรง\n",
    "records_to_insert = source_df_hashed.join(\n",
    "    dim_customer_current_hashed,\n",
    "    (source_df_hashed.CustomerID == dim_customer_current_hashed.CustomerID) & \\\n",
    "    (source_df_hashed.scd_hash == dim_customer_current_hashed.scd_hash), # ใช้ Hash ตรงกัน\n",
    "    \"left_anti\" # เอาเฉพาะ records ที่ไม่เจอ (new) หรือเจอแต่ Hash ไม่ตรง (changed)\n",
    ").select(\n",
    "    col(\"CustomerID\"),\n",
    "    col(\"CustomerName\"),\n",
    "    col(\"CustomerAddress\"),\n",
    "    col(\"CustomerPhone\"),\n",
    "    # ... คอลัมน์อื่นๆ\n",
    "    col(\"scd_hash\")\n",
    ") \\\n",
    ".withColumn(\"StartDate\", current_date_val) \\\n",
    ".withColumn(\"EndDate\", lit(\"9999-12-31\").cast(\"date\")) \\\n",
    ".withColumn(\"IsCurrent\", lit(True))\n",
    "\n",
    "# 4. Insert Records ใหม่ (ทั้ง New และ New Version of Changed)\n",
    "if records_to_insert.count() > 0:\n",
    "    records_to_insert.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .saveAsTable(\"dim_customer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8bec9208-26eb-4ced-9999-0209214bf2cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**4. ข้อควรพิจารณาเพิ่มเติม**\n",
    "\n",
    "- Performance ของ Hash: การ Hash เป็นวิธีที่ดี แต่ต้องแน่ใจว่าการรวม String และการคำนวณ Hash ไม่เป็น Bottleneck หากมีหลายร้อยคอลัมน์ที่ต้อง Hash อาจจะต้องพิจารณา Subset ของคอลัมน์ที่สำคัญจริงๆ\n",
    "\n",
    "- Hash Collision: โอกาสเกิด Hash Collision (สอง String ที่ต่างกันได้ Hash Value เดียวกัน) มีน้อยมากสำหรับ MD5/SHA256 แต่ก็เป็นไปได้ในทางทฤษฎี สำหรับข้อมูลที่มีความสำคัญสูงมากๆ อาจจะต้องพิจารณาความเสี่ยง\n",
    "\n",
    "- การจัดการ NULL Values: ย้ำอีกครั้งว่าการจัดการ NULL ในคอลัมน์ที่จะ Hash ต้องสอดคล้องกัน มิฉะนั้นการเปลี่ยนแปลงจาก NULL เป็นค่า หรือจากค่าเป็น NULL จะไม่ถูกตรวจจับ หรือ Hash จะไม่ตรงกันเมื่อไม่ควรจะเป็น\n",
    " \n",
    "- เช่น concat_ws(\"||\", coalesce(col(\"col1\"), lit(\"NULL\")), coalesce(col(\"col2\"), lit(\"NULL\")))\n",
    "\n",
    "- Column Order: ตรวจสอบให้แน่ใจว่าลำดับของคอลัมน์ที่ใช้ในการ concat_ws (หรือการสร้าง Hash) นั้น เหมือนกันทั้งใน Source และ Destination มิฉะนั้น Hash จะไม่ตรงกันแม้ข้อมูลจะเหมือนกัน\n",
    "\n",
    "- Datatype Consistency: ตรวจสอบว่า Datatype ของคอลัมน์ที่จะ Hash เหมือนกันทั้ง Source และ Destination\n",
    "\n",
    "- Non-SCD Attributes: คอลัมน์ที่ไม่ติดตามการเปลี่ยนแปลง (เช่น LastUpdatedTimestamp ของ Source) ควรถูก UPDATE ลงใน Record ปัจจุบันของ Destination (ที่ IsCurrent = True) โดยไม่สร้าง Record ใหม่\n",
    "\n",
    "- ใช้ Delta Lake: SCD Type 2 มีประสิทธิภาพสูงสุดเมื่อใช้ Delta Lake เป็น Destination Table เพราะรองรับ MERGE INTO และ ACID transactions\n",
    "\n",
    "การใช้ Hashing เป็น Best Practice ที่แข็งแกร่งสำหรับการทำ SCD Type 2 ครับ มันช่วยลดความซับซ้อนของการเปรียบเทียบหลายคอลัมน์ และทำให้โค้ดมีความยืดหยุ่นมากขึ้นเมื่อมีการเพิ่ม/ลบคอลัมน์ SCD ในอนาคตครับ\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Example_SCD_Type2_Imperative_code",
   "widgets": {}
  },
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "9a44a580-faf3-4ed6-8799-d5e62995802f",
    "default_lakehouse_name": "databricks_lakehouse",
    "default_lakehouse_workspace_id": "a0d3bec5-cae8-4f46-bf6e-2d70272b3718",
    "known_lakehouses": [
     {
      "id": "9a44a580-faf3-4ed6-8799-d5e62995802f"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
