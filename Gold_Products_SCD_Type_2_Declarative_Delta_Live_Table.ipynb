{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99e4d483-792c-4b78-912f-0e429628cec3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **DLT Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0eaa962d-a343-4ce0-a456-11dc36999fd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0788eef-ceda-4632-ad73-03f28bf4c2ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Streaming Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68565748-0934-4764-af9a-da055a456394",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Expectations\n",
    "# คล้ายๆกับการเขียน SQL\n",
    "my_rules = {\n",
    "    \"rule1\": \"product_id IS NOT NULL\",\n",
    "    \"rule2\": \"product_name IS NOT NULL\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "715b5f75-c86b-44be-a3f9-9d8aa5e23676",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Expectations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "926782bb-27c9-4eab-9cf4-ea15502952bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table()\n",
    "\n",
    "@dlt.expect_all_or_drop(my_rules)\n",
    "def DimProducts_stage():\n",
    "  df = spark.readStream.table(\"databricks_cata.silver.products_silver\")\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "518ba295-8b94-4cbe-b18a-77f4987dc961",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Streaming View**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70659a42-78d7-47d9-bf68-0cb13a36baf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.view()\n",
    "def DimProducts_View():\n",
    "    # ถึงแม้ Table DimProducts_stage จะยังไม่ถูกสร้างขึ้นจริง(Cell ด้านบน) แต่เราสามารถ refer ไปที่ตารางนั้นได้ด้วย Live.<table_name>\n",
    "    df = dlt.read_stream.table(\"Live.DimProducts_stage\")\n",
    "    return df\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f94912c1-4a59-486a-b1d5-18b499065ac4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "การเตรียมข้อมูลใน Delta Live Tables (DLT) โดยการสร้าง **@dlt.table()** เพื่ออ่านข้อมูลแบบ streaming ก่อน แล้วค่อยสร้าง** @dlt.view() **มีเหตุผลหลักๆ ดังนี้:\n",
    "> \n",
    "1. การจัดการข้อมูลแบบ Streaming: การใช้ **@dlt.table() **เพื่ออ่านข้อมูลแบบ streaming ช่วยให้คุณสามารถจัดการกับข้อมูลที่เข้ามาอย่างต่อเนื่องได้อย่างมีประสิทธิภาพ โดยข้อมูลจะถูกประมวลผลทีละ record ซึ่งเหมาะสำหรับการทำงานกับแหล่งข้อมูลที่มีการเพิ่มข้อมูลใหม่อย่างต่อเนื่อง เช่น Kafka, Event Hubs หรือไฟล์ที่ถูกเพิ่มเข้ามาใน cloud storage\n",
    "\n",
    "2. การตรวจสอบคุณภาพข้อมูล: การใช้ **@dlt.expect_all_or_drop(my_rules)** ใน **@dlt.table() **ช่วยให้คุณสามารถกำหนดกฎการตรวจสอบคุณภาพข้อมูลได้ โดยข้อมูลที่ไม่ผ่านกฎจะถูกดรอปออกไป ทำให้ข้อมูลที่เข้าสู่ขั้นตอนถัดไปมีคุณภาพที่ดีขึ้น\n",
    "\n",
    "3. การแยกขั้นตอนการประมวลผล: การสร้าง **@dlt.view()** หลังจาก **@dlt.table()** ช่วยให้คุณสามารถแยกขั้นตอนการประมวลผลออกเป็นหลายๆ ขั้นตอน ทำให้โค้ดมีความชัดเจนและง่ายต่อการบำรุงรักษา โดย **@dlt.view()** จะทำหน้าที่เป็นการแปลงข้อมูลหรือการทำงานเพิ่มเติมที่ไม่ต้องการให้ข้อมูลถูกเก็บในตารางจริง\n",
    "\n",
    "4. การใช้ประโยชน์จากการประมวลผลแบบ Batch และ Streaming: การใช้ **@dlt.table()** และ **@dlt.view()** ร่วมกันช่วยให้คุณสามารถใช้ประโยชน์จากการประมวลผลทั้งแบบ batch และ streaming ได้ โดย **@dlt.table()** จะทำหน้าที่เป็นตารางที่เก็บข้อมูลแบบ streaming และ **@dlt.view() **จะทำหน้าที่เป็นการแปลงข้อมูลหรือการทำงานเพิ่มเติมที่สามารถใช้ได้ทั้งในแบบ batch และ streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b7a3e1b-2997-4e43-84c6-1c1204d27696",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**DimProducts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed57f044-9481-4591-bef9-7c0502c5afb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dlt.create_streaming_table(\"DimProducts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9ef2e21-cc96-4135-9b22-f21fd24ceb56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# คำสั่ง apply_changes จะต้องทำงานกับ Delta Table ที่เป็น Streaming (หรือ Live Table)\n",
    "dlt.apply_changes(\n",
    "\n",
    "    target = \"DimProducts\",\n",
    "    source = \"Live.DimProducts_View\",\n",
    "    keys = [\"product_id\"],  # Primary key ของตาราง Source\n",
    "    sequence_by = \"product_id\", # ควรที่จะเป็นคอลัมน์ Date ใน source แต่ถ้าไม่มีก็ให้เรียงลำดับตาม PK ก็ได้\n",
    "    # ignore_null_updates = False,    \n",
    "    # apply_as_deletes = None,\n",
    "    # apply_as_truncates = None,\n",
    "    # column_list = None,\n",
    "    # except_column_list = None,\n",
    "    stored_as_scd_type = 2, # SCD type 1 หรือ 2\n",
    "    # track_history_column_list = None,\n",
    "    # track_history_except_column_list = None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6d78bad-f970-41c1-bb07-de57c93b71fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**การใช้ create_streaming_table() เพื่อสร้างตารางเป้าหมายสำหรับ apply_changes() มีเหตุผลหลักๆ ดังนี้:**\n",
    "\n",
    "1. การจัดการข้อมูลแบบ Streaming: apply_changes() ใช้สำหรับการประมวลผลข้อมูลที่มีการเปลี่ยนแปลงอย่างต่อเนื่อง (Change Data Capture - CDC) ซึ่งต้องการตารางเป้าหมายที่รองรับการประมวลผลแบบ streaming เพื่อให้สามารถจัดการกับข้อมูลที่เข้ามาใหม่ได้อย่างมีประสิทธิภาพ\n",
    "\n",
    "2. การเก็บประวัติการเปลี่ยนแปลง: การทำ SCD Type 2 ต้องการการเก็บประวัติการเปลี่ยนแปลงของข้อมูล ซึ่ง create_streaming_table() ช่วยให้สามารถเก็บข้อมูลที่มีการเปลี่ยนแปลงได้อย่างถูกต้องและครบถ้วน\n",
    "\n",
    "\n",
    "3. การรองรับฟังก์ชันเฉพาะของ DLT: ฟังก์ชัน apply_changes() และ apply_changes_from_snapshot() ต้องการตารางเป้าหมายที่สร้างด้วย create_streaming_table() เพื่อให้สามารถทำงานได้อย่างถูกต้องและมีประสิทธิภาพ\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Gold_Products_SCD_Type_2_Declarative_Delta_Live_Table",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
